# -*- coding: utf-8 -*-
"""kobe(loc_x,loc_y)_(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12JGiui0rqfLLHuP42ILGd8Lu9XoUEoMf
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
sns.set(style='whitegrid', context='notebook')

# Cargamos una base de datos preexistente
kobedata = pd.read_csv('https://raw.githubusercontent.com/raguiso/kobebryant/main/data.csv', sep =',')

# Primero, nos aseguramos de que el valor de la season esté en formato numérico
kobedata['season_numeric'] = kobedata['season'].apply(lambda x: int(x.split('-')[0]))

# Ahora filtramos las temporadas deseadas
kobedata_filtered = kobedata[(kobedata['season_numeric'] >= 1998) & (kobedata['season_numeric'] <= 2012)]
#Eliminamos las columnas (categorías e indicadores) que no vamos a analizar
columns_to_drop_estudio = ['action_type', 'game_event_id', 'game_id', 'season_numeric', 'lat','lon', 'team_id', 'minutes_remaining','season','shot_zone_area','period','combined_shot_type',
                   'game_date', 'matchup', 'opponent', 'team_name', 'playoffs', 'seconds_remaining','shot_zone_range','shot_type','shot_distance','shot_id','shot_zone_basic']
kobedata_estudio = kobedata_filtered.drop(columns=columns_to_drop_estudio)


# Eliminar filas donde haya NaN
kobedata_estudio.dropna(inplace=True)

kobedata_estudio

print(min(kobedata_estudio['loc_x']))
print(max(kobedata_estudio['loc_x']))
print(min(kobedata_estudio['loc_y']))
print(max(kobedata_estudio['loc_y']))

# @title loc_y vs shot_made_flag

from matplotlib import pyplot as plt
kobedata_estudio.plot(kind='scatter', x='loc_y', y='shot_made_flag', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

kobedata_estudio.plot(kind='scatter', x='loc_x', y='shot_made_flag', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

plt.figure(figsize=(8,11))
corr = kobedata_estudio.corr()['shot_made_flag'].sort_values(ascending=True).reset_index()[0:2]
sns.barplot(data=corr, x='shot_made_flag',y='index')

X = kobedata_estudio.drop('shot_made_flag', axis=1)
y = kobedata_estudio['shot_made_flag'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2, random_state=444)

print(f"""X_train:{X_train.shape}
X_test:{X_test.shape}

y_train:{y_train.shape}
y_test:{y_test.shape}""")

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
result = model.fit(X_train,y_train)

from sklearn import metrics
pred_test = model.predict(X_test)
print(metrics.accuracy_score(y_test, pred_test))

print("clasifica bien el", metrics.accuracy_score(y_test, pred_test)*100, "%  de los datos")

print("Coeficientes:", list(zip(X.columns, model.coef_.flatten(), )))

from sklearn.metrics import confusion_matrix
conf_matrix = confusion_matrix(y_test, pred_test)

plt.figure(figsize=(6,4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Fallado', 'Encestado'], yticklabels=['Fallado', 'Encestado'])
plt.ylabel('Actual')
plt.xlabel('Predicción')
plt.title('Matriz de Confusión')
plt.show()

X = kobedata_estudio.drop('shot_made_flag', axis=1)
y = kobedata_estudio['shot_made_flag'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2, random_state=444)

import statsmodels.api as sm
import statsmodels.formula.api as smf

X_train = sm.add_constant(X_train, prepend=True)
modelo = sm.Logit(endog=y_train, exog=X_train,)
modelo = modelo.fit()
print(modelo.summary())

X, Y = np.meshgrid(np.linspace(-25, 25, 100),
                   np.linspace(-4, 92, 100))

def f(d): return 1/ (1 + np.exp(-0.3790 + 0.0437*d) )

Z = 1/ (1 + np.exp(-0.1120 + 0.0001*X + 0.0034*Y) )

# Contour relleno
contour = plt.contourf(X, Y, Z, cmap='plasma')
plt.gca().set_aspect(1)
cbar = plt.colorbar(contour)

"""SOLUCION: USAR LA DISTANCIA A LA CANCHA"""

# Cargamos una base de datos preexistente
kobedata = pd.read_csv('https://raw.githubusercontent.com/raguiso/kobebryant/main/data.csv', sep =',')

# Primero, nos aseguramos de que el valor de la season esté en formato numérico
kobedata['season_numeric'] = kobedata['season'].apply(lambda x: int(x.split('-')[0]))

# Ahora filtramos las temporadas deseadas
kobedata_filtered = kobedata[(kobedata['season_numeric'] >= 1998) & (kobedata['season_numeric'] <= 2012)]
#Eliminamos las columnas (categorías e indicadores) que no vamos a analizar
columns_to_drop_estudio = ['action_type', 'game_event_id', 'game_id', 'season_numeric', 'loc_x', 'loc_y', 'lat','lon', 'team_id', 'minutes_remaining','season','shot_zone_area','period', 'combined_shot_type',
                   'game_date', 'matchup', 'opponent', 'team_name', 'playoffs', 'seconds_remaining','shot_zone_range','shot_type','shot_id','shot_zone_basic']
kobedata_estudio = kobedata_filtered.drop(columns=columns_to_drop_estudio)


# Eliminar filas donde haya NaN
kobedata_estudio.dropna(inplace=True)

kobedata_estudio

type(kobedata_estudio['shot_distance'][1])

# Cargamos una base de datos preexistente
kobedata = pd.read_csv('https://raw.githubusercontent.com/raguiso/kobebryant/main/data.csv', sep =',')

# Primero, nos aseguramos de que el valor de la season esté en formato numérico
kobedata['season_numeric'] = kobedata['season'].apply(lambda x: int(x.split('-')[0]))

# Ahora filtramos las temporadas deseadas
kobedata_filtered = kobedata[(kobedata['season_numeric'] >= 1998) & (kobedata['season_numeric'] <= 2012)]
#Eliminamos las columnas (categorías e indicadores) que no vamos a analizar
columns_to_drop_estudio = ['action_type', 'game_event_id', 'game_id', 'season_numeric', 'combined_shot_type', 'lat','lon', 'team_id', 'minutes_remaining','season','shot_zone_area','period',
                   'game_date', 'matchup', 'opponent', 'team_name', 'playoffs', 'seconds_remaining','shot_zone_range','shot_type','shot_id','shot_zone_basic', 'loc_x', 'loc_y']
kobedata_estudio = kobedata_filtered.drop(columns=columns_to_drop_estudio)


# Eliminar filas donde haya NaN
kobedata_estudio.dropna(inplace=True)

kobedata_estudio

max(kobedata_estudio['shot_distance'])

X = kobedata_estudio.drop('shot_made_flag', axis=1)
y = kobedata_estudio['shot_made_flag'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2, random_state=444)

print(f"""X_train:{X_train.shape}
X_test:{X_test.shape}

y_train:{y_train.shape}
y_test:{y_test.shape}""")

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
result = model.fit(X_train,y_train)

from sklearn import metrics
pred_test = model.predict(X_test)
print(metrics.accuracy_score(y_test, pred_test))

print("clasifica bien el", metrics.accuracy_score(y_test, pred_test)*100, "%  de los datos")

X = kobedata_estudio.drop('shot_made_flag', axis=1)
y = kobedata_estudio['shot_made_flag'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2, random_state=444)

import statsmodels.api as sm
import statsmodels.formula.api as smf

X_train = sm.add_constant(X_train, prepend=True)
modelo = sm.Logit(endog=y_train, exog=X_train,)
modelo = modelo.fit()
print(modelo.summary())

1/(1+ np.exp(-0.3790))

from matplotlib import pyplot as plt
kobedata_estudio.plot(kind='scatter', x='shot_distance', y='shot_made_flag', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

x = np.linspace(-4,80)
def f(z): return 1 / (1 + np.exp(-0.3790+0.0437*z))
plt.plot(x, f(x), color='red')

# Datos
X, Y = np.meshgrid(np.linspace(-25, 25, 100),
                   np.linspace(-4, 92, 100))

def f(d): return 1/ (1 + np.exp(-0.3790 + 0.0437*d) )

Z = f(np.sqrt(X**2 + Y**2))

# Contour relleno
contour = plt.contourf(X, Y, Z, cmap='plasma', levels=100)
plt.gca().set_aspect(1)
cbar = plt.colorbar(contour)

1/(1 + np.exp(-0.3790+0.0437*92))

X, Y = np.meshgrid(np.linspace(-25, 25, 100),
                   np.linspace(-4, 92, 100))

Z1 = 1/ (1 + np.exp(-0.1120 + 0.0001*X + 0.0034*Y) )

def f(d): return 1/ (1 + np.exp(-0.3790 + 0.0437*d) )

Z2 = f(np.sqrt(X**2 + Y**2))


fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

# Crear el primer mapa de calor
contour1 = ax1.contourf(X, Y, Z1, cmap='tab20b', vmin=0.05, vmax=0.7)
cbar = fig.colorbar(contour1, ax=[ax1,ax2], orientation='vertical')

# Crear el segundo mapa de calor
contour2 = ax2.contourf(X, Y, Z2, cmap='tab20b', vmin=0.05, vmax=0.7)

# Ajustar la relación de aspecto si es necesario
ax1.set_aspect(1)
ax2.set_aspect(1)

# Agregar una única barra de colores que cubra ambos gráficos
# Debemos especificar el rango de valores que debe cubrir la barra de colores
cbar = fig.colorbar(contour2, ax=[ax1,ax2], orientation='vertical')

pip install palettable

from palettable.cmocean.diverging import Balance_20
from palettable.cmocean.sequential import Thermal_20
from palettable.mycarta import Cube1_20

X, Y = np.meshgrid(np.linspace(-25, 25, 100),
                   np.linspace(-4, 92, 100))

def f(d): return 1/ (1 + np.exp(-0.3790 + 0.0437*d) )

Z1 = f(np.sqrt(X**2 + Y**2))

Z2 = 1/ (1 + np.exp(-0.1120 + 0.0001*X + 0.0034*Y) )


fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

# Crear el primer mapa de calor
contour1 = ax1.contourf(X, Y, Z1,cmap=Balance_20.mpl_colormap,vmin=0, vmax=0.7)


# Crear el segundo mapa de calor
contour2 = ax2.contourf(X, Y, Z2, cmap=Balance_20.mpl_colormap, vmin=0, vmax=0.7)

# Ajustar la relación de aspecto si es necesario
ax1.set_aspect(1)
ax2.set_aspect(1)

ax1.set_title("Ejemplo 1")
ax2.set_title("Ejemplo 2")



# Agregar una única barra de colores que cubra ambos gráficos
# Debemos especificar el rango de valores que debe cubrir la barra de colores
cbar = fig.colorbar(contour1, ax=[ax1,ax2], orientation='vertical')